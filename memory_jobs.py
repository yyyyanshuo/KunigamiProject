import os
import json
import sqlite3
import datetime
from datetime import timedelta
# --- ã€ä¿®æ”¹ã€‘è¿™é‡ŒåŠ ä¸Š update_short_memory_for_date ---
from app import call_ai_to_summarize, update_short_memory_for_date

# å®šä¹‰è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROMPTS_DIR = os.path.join(BASE_DIR, "prompts")
SHORT_FILE = os.path.join(PROMPTS_DIR, "6_memory_short.json")
MEDIUM_FILE = os.path.join(PROMPTS_DIR, "5_memory_medium.json")
LONG_FILE = os.path.join(PROMPTS_DIR, "4_memory_long.json")
DATABASE_FILE = os.path.join(BASE_DIR, "chat_history.db") # æ•°æ®åº“è·¯å¾„

def process_daily_rollover(target_date_str=None):
    """
    æ—¥ç»“æµç¨‹ï¼š
    1. è‡ªåŠ¨è¡¥å½•ï¼šæ£€æŸ¥æ˜¨å¤©è¿˜æœ‰æ²¡æœ‰æœªæ€»ç»“çš„æ¶ˆæ¯ï¼Œæœ‰çš„è¯å…ˆæ€»ç»“è¿› Shortã€‚
    2. æ±‡æ€»ï¼šæŠŠ Short é‡Œæ˜¨å¤©çš„æ‰€æœ‰äº‹ä»¶ï¼Œåˆå¹¶æˆä¸€ç¯‡ Medium æ—¥è®°ã€‚
    """
    # é»˜è®¤å¤„ç†æ˜¨å¤©
    if not target_date_str:
        target_date_str = (datetime.datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    print(f"â° [å®šæ—¶ä»»åŠ¡] å¼€å§‹æ—¥ç»“æµç¨‹: {target_date_str}")

    # --- æ­¥éª¤ 1: è‡ªåŠ¨è¡¥æ¼ (Catch-up) ---
    print(f"   -> 1. æ£€æŸ¥æ˜¯å¦æœ‰æœªæ€»ç»“çš„æ®‹ç•™æ¶ˆæ¯...")
    try:
        # è°ƒç”¨ app.py é‡Œçš„å¢é‡æ›´æ–°å‡½æ•°ï¼ŒæŠŠæ˜¨å¤©å‰©ä¸‹çš„å…¨å¤„ç†äº†
        count, _ = update_short_memory_for_date(target_date_str)
        if count > 0:
            print(f"      âœ… è‡ªåŠ¨è¡¥å½•å®Œæˆï¼Œè¿½åŠ äº† {count} æ¡è®°å¿†ã€‚")
        else:
            print(f"      - æ— éœ€è¡¥å½•ã€‚")
    except Exception as e:
        print(f"      âŒ è¡¥å½•å‡ºé”™: {e}")

    # --- æ­¥éª¤ 2: å¼€å§‹ç”Ÿæˆä¸­æœŸè®°å¿† (Medium) ---
    print(f"   -> 2. ç”Ÿæˆæ—¥è®° (Short -> Medium)...")

    if not os.path.exists(SHORT_FILE): return

    with open(SHORT_FILE, "r", encoding="utf-8") as f:
        try: short_data = json.load(f)
        except: return

    # è·å–æ•°æ® (å…¼å®¹æ–°æ—§æ ¼å¼)
    day_data = short_data.get(target_date_str)
    events = []
    if isinstance(day_data, list):
        events = day_data
    elif isinstance(day_data, dict):
        events = day_data.get("events", [])

    if not events:
        print(f"      - {target_date_str} æ²¡æœ‰ä»»ä½•çŸ­æœŸè®°å¿†ï¼Œè·³è¿‡æ—¥ç»“ã€‚")
        return

    # æ‹¼å‡‘å®Œæ•´æ–‡æœ¬ (æŠŠè¿™ä¸€å¤©ç´¯ç§¯çš„æ‰€æœ‰äº‹ä»¶éƒ½ç»™ AI)
    text_to_summarize = "\n".join([f"[{e['time']}] {e['event']}" for e in events])
    summary = call_ai_to_summarize(text_to_summarize, "medium")

    if not summary: return

    # å†™å…¥ Medium
    medium_data = {}
    if os.path.exists(MEDIUM_FILE):
        with open(MEDIUM_FILE, "r", encoding="utf-8") as f:
            try: medium_data = json.load(f)
            except: pass

    medium_data[target_date_str] = summary

    with open(MEDIUM_FILE, "w", encoding="utf-8") as f:
        json.dump(medium_data, f, ensure_ascii=False, indent=2)

    print("      ğŸ“ æ—¥è®°å†™å…¥å®Œæˆã€‚")
    print("âœ… æ—¥ç»“æµç¨‹ç»“æŸã€‚")

def process_weekly_rollover():
    print("â° [å®šæ—¶ä»»åŠ¡] å¼€å§‹å‘¨ç»“...")
    if not os.path.exists(MEDIUM_FILE): return

    with open(MEDIUM_FILE, "r", encoding="utf-8") as f:
        try: medium_data = json.load(f)
        except: return

    today = datetime.datetime.now()
    summary_buffer = []

    # è¿‡å»7å¤©
    for i in range(7):
        d = (today - timedelta(days=i)).strftime('%Y-%m-%d')
        if d in medium_data:
            summary_buffer.append(f"ã€{d}ã€‘: {medium_data[d]}")

    if not summary_buffer: return

    full_text = "\n".join(summary_buffer)
    long_summary = call_ai_to_summarize(full_text, "long")

    if not long_summary: return

    week_key = f"{today.strftime('%Y-%m')}-Week{ (today.day - 1) // 7 + 1}"

    long_data = {}
    if os.path.exists(LONG_FILE):
        with open(LONG_FILE, "r", encoding="utf-8") as f:
            try: long_data = json.load(f)
            except: pass

    long_data[week_key] = long_summary

    with open(LONG_FILE, "w", encoding="utf-8") as f:
        json.dump(long_data, f, ensure_ascii=False, indent=2)

    print("   - ğŸ“œ å‘¨ç»“(Long)å†™å…¥å®Œæˆã€‚")